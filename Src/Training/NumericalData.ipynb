{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Numerical APTOS2019 Dataset Notebook** \n",
    "###### _Dataset obtained from: https://www.kaggle.com/datasets/mariaherrerot/aptos2019/data?select=valid.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import io, color\n",
    "from skimage.feature import local_binary_pattern\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the first 5 rows of the 'train' csv file\n",
    "\n",
    "feature_extraction_csv_file_path = os.path.join('..', '..', 'Data', 'APTOS-2019 Dataset', 'high_res_features_train.csv')\n",
    "\n",
    "feature_extraction_df = pd.read_csv(feature_extraction_csv_file_path)\n",
    "\n",
    "feature_extraction_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distrobution of the training data is given below\n",
    "\n",
    "feature_extraction_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining if there are any null values\n",
    "feature_extraction_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "display(feature_extraction_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of each column\n",
    "print(\"\\nData types of each column:\")\n",
    "display(feature_extraction_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for numerical features\n",
    "print(\"\\nHistograms for numerical features:\")\n",
    "numerical_columns = ['Exudates', 'Oedema', 'Vessel Count', 'Heamotomas', 'Diabetes Status']\n",
    "feature_extraction_df[numerical_columns].hist(bins=15, figsize=(10, 6), layout=(2, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each numerical feature, separating by 'Diabetes Status'\n",
    "print(\"\\nHistograms for numerical features:\")\n",
    "\n",
    "# List of numerical columns (excluding the target column 'Diabetes Status')\n",
    "numerical_columns = ['Exudates', 'Oedema', 'Vessel Count', 'Heamotomas']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each numerical column and plot histograms for 'Diabetes Status = 0' and 'Diabetes Status = 1'\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    axes[i].hist(\n",
    "        feature_extraction_df[feature_extraction_df['Diabetes Status'] == 0][col], \n",
    "        bins=15, alpha=0.7, label='Non-Diabetic (0)', color='blue'\n",
    "    )\n",
    "    axes[i].hist(\n",
    "        feature_extraction_df[feature_extraction_df['Diabetes Status'] == 1][col], \n",
    "        bins=15, alpha=0.7, label='Diabetic (1)', color='orange'\n",
    "    )\n",
    "    axes[i].set_title(f'{col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations between features\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "correlation_matrix = feature_extraction_df[numerical_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for class distribution in the target column (e.g., 'Diabetes Status')\n",
    "print(\"\\nClass distribution of 'Diabetes Status':\")\n",
    "print(feature_extraction_df['Diabetes Status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between 'Diabetes Status' and other features\n",
    "sns.pairplot(feature_extraction_df, hue='Diabetes Status', vars=numerical_columns[:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (replace 'path_to_your_csv_file.csv' with the actual file path)\n",
    "#data = os.path.join('..', '..', 'Data', 'APTOS-2019 Dataset', 'features_500_res.csv')\n",
    "train_data = os.path.join('..', '..', 'Data', 'APTOS-2019 Dataset', 'high_res_features_train.csv')\n",
    "test_data = os.path.join('..', '..', 'Data', 'APTOS-2019 Dataset', 'high_res_features_test.csv')\n",
    "\n",
    "train_data = pd.read_csv(train_data)\n",
    "test_data = pd.read_csv(test_data)\n",
    "\n",
    "# Step 2: Separate features (X) and labels (y)\n",
    "X = train_data.drop(columns=['Image Name', 'Diabetes Status'])  # Drop 'Image Name' as it's not a useful feature\n",
    "y = train_data['Diabetes Status']  # This is the target variable\n",
    "\n",
    "X_test = test_data.drop(columns=['Image Name', 'Diabetes Status'])  # Drop 'Image Name' as it's not a useful feature\n",
    "y_test = test_data['Diabetes Status']  # This is the target variable\n",
    "\n",
    "# Step 3: Split the data into train and test sets (70% train, 30% test)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Train set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Standardize the numerical features\n",
    "scaler = StandardScaler() \n",
    "\n",
    "# Fit the scaler on the training data and transform both the train and val data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "x_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# Optional: Convert the scaled data back into DataFrame (for readability purposes)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns)\n",
    "\n",
    "x_test_scaled = pd.DataFrame(x_test_scaled, columns=X.columns)\n",
    "\n",
    "# Display the first few rows of the scaled training data\n",
    "print(\"\\nScaled training data:\")\n",
    "display(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Oedema from the standardised dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'Oedema' feature from both the training and validation sets\n",
    "X_train_scaled = X_train_scaled.drop(columns=['Oedema'])\n",
    "X_val_scaled = X_val_scaled.drop(columns=['Oedema'])\n",
    "x_test_scaled = x_test_scaled.drop(columns=['Oedema'])\n",
    "\n",
    "# Display the first few rows of the updated training data\n",
    "print(\"\\nTraining data after removing 'Oedema':\")\n",
    "display(X_train_scaled.head())\n",
    "\n",
    "# Display the first few rows of the updated validation data\n",
    "print(\"\\nValidation data after removing 'Oedema':\")\n",
    "display(X_val_scaled.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Oedema from the Original Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'Oedema' feature from both the training and validation sets\n",
    "X_train = X_train.drop(columns=['Oedema'])\n",
    "X_val = X_val.drop(columns=['Oedema'])\n",
    "X_test = X_test.drop(columns=['Oedema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reuseable Functions for Plotting Model Statistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Plotting Classifier Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tree-based classifiers, plots the confusion matrix, histogram of feature importance and ROC + Precision-Recall Curves\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_model_evaluation_statistics(model, X_val, y_val):\n",
    "    # Predict probabilities\n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.show()\n",
    "\n",
    "    # Feature Importance\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    feature_importances = model.feature_importances_\n",
    "    indices = np.argsort(feature_importances)[::-1] # Sorts feature importances in descending order and get the indices\n",
    "    feature_names_sorted = X_val.columns[indices]  # Arranges feature names according to the indices\n",
    "\n",
    "    plt.title('Feature Importances')\n",
    "    plt.bar(range(X_val.shape[1]), feature_importances[indices], color=\"r\", align=\"center\")\n",
    "    plt.xticks(range(X_val.shape[1]), feature_names_sorted, rotation=90)\n",
    "    plt.xlim([-1, X_val.shape[1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Linear-based Classifiers, plots the confusion matrix, histogram of feature importance and ROC + Precision-Recall Curves\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_linear_model_evaluation_statistics(model, X_val, y_val):\n",
    "    # Predict probabilities\n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Predict class labels\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.show()\n",
    "\n",
    "    # Coefficients for Linear Model\n",
    "    if hasattr(model, 'coef_'):\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        coefficients = model.coef_[0]\n",
    "        indices = np.argsort(np.abs(coefficients))[::-1]\n",
    "        feature_names_sorted = X_val.columns[indices]\n",
    "\n",
    "        plt.title('Feature Coefficients')\n",
    "        plt.bar(range(len(indices)), coefficients[indices], color=\"r\", align=\"center\")\n",
    "        plt.xticks(range(len(indices)), feature_names_sorted, rotation=90)\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Coefficient Value')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Plotting Regressor Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the same plots as done for the classifier models, however, also plots the histogram of predictions and uses the cut-off threshold to make predictions binary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, accuracy_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_regression_evaluation_statistics(model, X_val, y_val, cutoff_threshold=0.5):\n",
    "    # Continuous predictions\n",
    "    y_pred_continuous = model.predict(X_val)\n",
    "\n",
    "    # Convert continuous predictions to binary using the cutoff threshold\n",
    "    y_pred_binary = np.where(y_pred_continuous > cutoff_threshold, 1, 0)\n",
    "\n",
    "    # Calculate accuracy for the binary predictions\n",
    "    accuracy = accuracy_score(y_val, y_pred_binary)\n",
    "    print(f\"Validation Accuracy (with cutoff {cutoff_threshold}): {accuracy:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    cm = confusion_matrix(y_val, y_pred_binary)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_continuous)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_continuous)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.show()\n",
    "\n",
    "    # Feature Importance (if model supports it - Bayes does not)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    feature_importances = model.feature_importances_\n",
    "    indices = np.argsort(feature_importances)[::-1]\n",
    "    feature_names_sorted = X_val.columns[indices]\n",
    "\n",
    "    plt.title('Feature Importances')\n",
    "    plt.bar(range(X_val.shape[1]), feature_importances[indices], color=\"r\", align=\"center\")\n",
    "    plt.xticks(range(X_val.shape[1]), feature_names_sorted, rotation=90)\n",
    "    plt.xlim([-1, X_val.shape[1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Plotting Neural Network Statistics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the confusion matrix, ROC + Area Under Curve graph, precision-recall curve as well as the loss and accuracy graphs as epochs increase.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def analyze_neural_network_performance(model, X_train, y_train, X_val, y_val, history=None):\n",
    "    # Predict probabilities for the validation set\n",
    "    y_pred_prob = model.predict(X_val).squeeze()\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(recall, precision, color='blue', lw=2)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    print(classification_report(y_val, y_pred, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "    # Training Process: Loss and Accuracy\n",
    "    if history is not None:\n",
    "        # Plot training & validation accuracy values\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Val'], loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a logistic regression model and testing it's accuracy on the validation data\n",
    "\n",
    "LR_model = LogisticRegression(max_iter=1000) # instantiate the model\n",
    "\n",
    "#model.fit(X_train, y_train) # fitting the model on the training set\n",
    "LR_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = LR_model.predict(X_val_scaled) # making predictions on the validation set\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred) # calculate the accuracy of the model\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning using Optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Objective function to optimize with Optuna\n",
    "def objective(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    solver = trial.suggest_categorical('solver', ['liblinear', 'saga', 'lbfgs'])\n",
    "    C = trial.suggest_float('C', 1e-6, 1e3, log=True)  # Regularization strength\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 2000)\n",
    "    \n",
    "    # Create the Logistic Regression model with suggested hyperparameters\n",
    "    model = LogisticRegression(solver=solver, C=C, max_iter=max_iter, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation to get an evaluation score\n",
    "    score = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy').mean()\n",
    "    \n",
    "    # Append the validation accuracy for each trial to the list\n",
    "    validation_accuracies.append(score)\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Create a list to store validation accuracies for each trial\n",
    "validation_accuracies = []\n",
    "\n",
    "# Create the Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=600)\n",
    "\n",
    "# Output the best hyperparameters found by Optuna\n",
    "print(\"Best hyperparameters:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Train the Logistic Regression model using the best hyperparameters\n",
    "best_params = study.best_params\n",
    "LR_model_HP = LogisticRegression(**best_params, random_state=42)\n",
    "LR_model_HP.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = LR_model_HP.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy with tuned hyperparameters: {accuracy:.4f}\")\n",
    "\n",
    "# Plot the optimization history (from Optuna)\n",
    "opt_history_fig = plot_optimization_history(study)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "# Plot validation accuracy over time (custom plot)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, len(validation_accuracies) + 1), validation_accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.title('Validation Accuracy Over Trials')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_linear_model_evaluation_statistics(LR_model_HP, X_val_scaled, y_val)\n",
    "plot_linear_model_evaluation_statistics(LR_model, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_test_pred = LR_model.predict(x_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Testing Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_model_evaluation_statistics(LR_model, x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Random Forrest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Randdom Forrest Classifier: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a random forest model and testing it's accuracy on the validation data\n",
    "\n",
    "random_forest_model = RandomForestClassifier(n_estimators=1000, random_state=42)  # n_estimators is the number of trees\n",
    "\n",
    "random_forest_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = random_forest_model.predict(X_val_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_evaluation_statistics(random_forest_model, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_test_pred = random_forest_model.predict(x_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Testing Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_evaluation_statistics(random_forest_model, x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Random Forrest Regressor with Cutoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise random forrest regressor\n",
    "randForest_regressor = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "randForest_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on validation data\n",
    "y_pred = randForest_regressor.predict(X_val_scaled)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Calculate R^2 score\n",
    "r2_score = randForest_regressor.score(X_val_scaled, y_val)\n",
    "print(f\"Validation R^2 Score: {r2_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning using Optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Objective function for Optuna to minimize\n",
    "def objective(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "    \n",
    "    # Create the RandomForestRegressor model with suggested hyperparameters\n",
    "    randForest_regressor = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Perform cross-validation (you can adjust `cv` for more or fewer folds)\n",
    "    score = cross_val_score(randForest_regressor, X_train_scaled, y_train, cv=5, scoring='neg_root_mean_squared_error').mean()\n",
    "\n",
    "    # Return the negative RMSE (because Optuna maximizes by default)\n",
    "    return score\n",
    "\n",
    "# Create the Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Output the best hyperparameters found by Optuna\n",
    "print(\"Best hyperparameters:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Train the RandomForestRegressor model using the best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = best_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate RMSE on the validation set\n",
    "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "print(f\"Validation RMSE with tuned hyperparameters: {rmse:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "r2_score = best_model.score(X_val_scaled, y_val)\n",
    "print(f\"Validation R² Score with tuned hyperparameters: {r2_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Predictions to Understand the Binary Clustering and Cutoff Point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of predictions\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(y_pred, bins=20, color='blue', alpha=0.7)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Predictions')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_threshold_val = np.median(y_pred)\n",
    "print(cutoff_threshold_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_predictions = np.where(y_pred > cutoff_threshold_val, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_evaluation_statistics(randForest_regressor, X_val_scaled, y_val, cutoff_threshold=cutoff_threshold_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_test_pred = randForest_regressor.predict(x_test_scaled)\n",
    "\n",
    "plot_regression_evaluation_statistics(randForest_regressor, x_test_scaled, y_test, cutoff_threshold=cutoff_threshold_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Naive Bayes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Multinomial Naive Bayes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the model\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = mnb.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_model_evaluation_statistics(mnb, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "y_test_pred = mnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_model_evaluation_statistics(mnb, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Gaussian Naive Bayes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train the Naive Bayes model on the training set\n",
    "naive_bayes_model = GaussianNB()\n",
    "naive_bayes_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on testing data set\n",
    "y_pred = naive_bayes_model.predict(X_val_scaled)\n",
    "\n",
    "# Find the accuracy of the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_model_evaluation_statistics(naive_bayes_model, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on testing data set\n",
    "y_pred = naive_bayes_model.predict(x_test_scaled)\n",
    "\n",
    "# Find the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_model_evaluation_statistics(naive_bayes_model, x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. XGBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 XGBoost Classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on validation data\n",
    "y_pred = xgb_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter Tuning using Optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Objective function for Optuna to minimize\n",
    "def objective(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',  # Assuming binary classification\n",
    "        'eval_metric': 'logloss',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    }\n",
    "\n",
    "    # Create the XGBoost model with suggested hyperparameters\n",
    "    xgb_model = xgb.XGBClassifier(**param, use_label_encoder=False)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    score = cross_val_score(xgb_model, X_train_scaled, y_train, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "    return score\n",
    "\n",
    "# Create the Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Output the best hyperparameters found by Optuna\n",
    "print(\"Best hyperparameters:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Train the XGBoost model using the best hyperparameters\n",
    "best_params = study.best_params\n",
    "xgb_best_model = xgb.XGBClassifier(**best_params, use_label_encoder=False)\n",
    "xgb_best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = xgb_best_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy with tuned hyperparameters: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Statistics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_evaluation_statistics(xgb_best_model, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "y_test_pred = xgb_best_model.predict(x_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Validation Accuracy with tuned hyperparameters: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_evaluation_statistics(xgb_best_model, x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 XGBoost Regressor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost regressor\n",
    "xgb_regressor = xgb.XGBRegressor()\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on validation data\n",
    "y_pred = xgb_regressor.predict(X_val_scaled)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Calculate R^2 score\n",
    "r2_score = xgb_regressor.score(X_val_scaled, y_val)\n",
    "print(f\"Validation R^2 Score: {r2_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of predictions\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(y_pred, bins=20, color='blue', alpha=0.7)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Predictions')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_threshold_val = np.median(y_pred)\n",
    "print(cutoff_threshold_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_predictions = np.where(y_pred > cutoff_threshold_val, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_evaluation_statistics(xgb_regressor, X_val_scaled, y_val, cutoff_threshold=cutoff_threshold_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_test_pred = xgb_regressor.predict(x_test_scaled)\n",
    "\n",
    "plot_regression_evaluation_statistics(xgb_regressor, x_test_scaled, y_test, cutoff_threshold=cutoff_threshold_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Gradient Boosting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1 Gradient Boosting Classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 Gradient Boosting Regressor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. SVM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1 SVM with Linear Kernel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2 SVM with RBG Kernel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Combination Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SVM Model with rbg kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create the SVM model\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Step 2: Train the model on the scaled training data\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 3: Make predictions on the validation set\n",
    "y_val_pred = svm_model.predict(X_val_scaled)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 5: Print a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Step 6: Plot a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. SVM with Linear Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create the SVM model\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Step 2: Train the model on the scaled training data\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 3: Make predictions on the validation set\n",
    "y_val_pred = svm_model.predict(X_val_scaled)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 5: Print a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Step 6: Plot a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Extract and plot feature importance for the linear SVM\n",
    "# Extract the feature importance (absolute value of the coefficients)\n",
    "feature_importance = np.abs(svm_model.coef_).flatten()\n",
    "\n",
    "# Create a DataFrame to hold feature names and their importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort the features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Step 8: Plot the feature importance\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance in SVM (Linear Kernel)')\n",
    "plt.gca().invert_yaxis()  # Most important feature at the top\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InvestigationEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
